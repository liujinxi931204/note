redis cluster是redis的分布式解决方案  
### redis数据分区  
redis cluster采用虚拟槽分区，所有的键根据哈希函数映射到0~16383整数槽内，计算公式：slot=CRC16(key)&16383  
每个节点维护一部分槽以及槽所映射的键值数据  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/12/1602487937165-1602487937167.png)  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/12/1602487971287-1602487971288.png)  
redis虚拟槽分区的特点  
+ 解耦数据和节点之间的关系，简化了节点扩容和收缩难度  
+ 节点自身维护槽的映射关系，不需要客户端或者代理服务维护槽分区元数据  
+ 支持节点、槽、键之间的映射查询，用于数据路由、在线伸缩等场景  
集群功能限制  
redis集群相对单机在功能上存在一些限制  
+ key批量操作支持有限。如mset、mget，目前只支持具有相同slot值得key执行批量操作。对于映射为不同slot值得key由于执行mget、mset等操作可能存在多个节点上因此不被支持  
+ key事务操作支持有限。同理只支持多key在同一节点上的事务操作，当多个key分布在不同的节点上时无法使用事务功能  
+ key作为数据分区的最小粒度，因此不能将一个大的键值对象如hash、list等映射到不同的节点  
+ 不支持多数据库空间。单机下的redis可以支持16个数据库，集群模式下只能使用一个数据库空间，即db0  
+ 复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构  
## 搭建集群  
### 准备节点  
集群模式最小需要6个redis节点  
集群的配置文件如下：  
```shell
# 后台运行
daemonize yes 
#节点端口
port 6379
# 日志目录
dir "/search/odin/redis/log"
# 日志名称
logfile "6379.log"
# 数据文件
dbfilename "cluster-6379.rdb"
# 开启集群模式
cluster-enabled yes 
# 节点超时时间，单位毫秒
cluster-node-timeout 15000
# 集群内部配置文件
cluster-config-file "nodes-6379.conf"
```
如果第一次启动时没有集群内部配置文件，会自动创建，文件由cluster-config-file控制，不要手动去修改这个文件  
这个文件记录集群的初始状态，包括集群内部唯一ID  
### 节点握手  
节点握手是指一批运行在集群模式下的节点通过Gossip协议彼此通信，达到感知对方的过程。节点握手是集群彼此通信的第一步，由客户端发起命令  
`cluster meet {ip} {port}`  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/12/1602492623132-1602492623136.png)  
+ 节点6379本地创建6380节点信息对象，并发送meet消息  
+ 节点6380接受到meet消息后，保存6379节点信息并回复pong消息  
+ 之后节点6379和6380彼此定期通过ping/pong消息进行正常的节点通信  
只需要在集群内任意节点执行cluster meet {ip} {port}命令加入新节点，握手状态会通过消息在集群内传播  
握手成功后集群还不能正常工作，这时集群处于下线状态，所有的数据读写都被禁止  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/12/1602492990181-1602492990183.png)  
可以通过执行`cluster info`命令查看集群的状态  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/12/1602493066463-1602493066464.png)  
这是由于当前16384个槽全部没有分配到节点，集群无法完成槽到节点的映射，只有当这16384个槽全部分配给节点后，集群才进入在线状态  
### 分配槽  
redis集群把所有的数据映射到16384个槽中，每个key会映射为一个固定的槽，只有当节点分配了槽，才能响应和这些槽关联的键命令  
通过`cluster addslots {start_slots..end_slots}`命令分配槽  
分配完成之后，redis cluster的集群状态时OK，集群进入在线状态  
执行cluster nodes命令可以看到节点和槽的分配关系 
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/12/1602494231607-1602494231608.png)  
每个负责处理槽的节点应该具有从节点，保证当它出现故障时可以自动进行故障转移  
集群模式下，redis节点角色分为主节点和从节点，首次启动的节点和被分配槽的节点都是主节点，从节点负责复制主节点槽信息和相关数据  
在从节点上行执行`cluster replicate nodeID`使得该节点成为nodeId的从节点  
手动搭建集群比较复杂，redis官方提供了redis-trib.rb工具方便快速搭建集群  
## 节点通信 
### 通信流程  
redis集群采用p2p的Gossip(流言)协议，Gossip协议工作原理就是节点彼此不断通信，一段时间后所有的节点都会知道集群完整的信息  
通信过程说明  
1. 集群中的每个节点都会单独开辟一个TCP通道，用于节点之间彼此通信，通信端口在基础端口上加10000  
2. 每个节点在固定周期内通过特定规则选择几个节点发送ping命令  
3. 接收到ping消息的节点用pong消息作为响应  
### Gossip消息  
常用的Gossip消息可分为：ping消息、pong消息、meet消息、fail消息  
+ meet消息：用于通知新节点加入。消息发送者通知接收者加入到当前集群，meet消息通信正常完成后，接收节点会加入到集群中并进行周期ixng的ping、pong消息交换  
+ ping消息：集群内交换最频繁的消息，集群内每个节点每秒内向多个其他节点发送ping消息，用于检测节点是否在线和交换彼此状态消息。ping消息发送封装了自身节点和部分其他节点的状态数据  
+ pong消息：当接收到ping、meet消息时，作为响应消息回复给发送方确认消息正常通信。pong消息内部封装了自身状态数据，节点也可以向集群内广播自身的pong消息来通知整个集群对自身状态进行更新  
+ fail消息：当节点判定集群内另一个节点下线时，会向集群内广播一个fail消息，其他节点接收到fail消息后会把对应节点更新为下线状态  
### 节点选择  
通信节点选择的规则：  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/13/1602581109921-1602581109960.png)  
redis集群的Gossip协议需要兼顾信息交换实时性和成本开销  
#### 选择发送消息的节点数量  
+ 集群内每个节点维护定时任务每秒执行10次，每秒会随机选取5个节点找出最久没有通信的节点发送ping消息，用于保证Gossip消息交换的随机性  
+ 每100毫秒都会扫描本地节点列表，如果发现节点最近一次接受pong消息的时间大于cluster_node_timeout/2，则立刻发送ping消息，防止该节点信息太长时间没有更新  
+ 根据以上规则得出每个节点每秒需要发送ping消息的数量=1+10*num(node.pong_recevied>cluster_node_timeout/2)，因此cluster_node_timeout参数对消息发送的节点数量影响非常大  
#### 消息数据量  
+ 每个ping消息的数据量体现在消息头和消息体中，其中消息头主要占用空间的字段是2KB，这块空间占用相对固定  
+ 消息体会携带一定数量的其他节点信息用于交换，更大的集群每次消息通信的成本也就更高  
### 集群伸缩  
redis集群提供了灵活的节点扩容和缩容方案，在不影响集群对外服务的情况下，可以为集群添加节点进行扩容也可以下线部分节点进行缩容  
其中原理可以抽象为槽和对应数据在不同节点之间灵活移动  
#### 新节点  
新节点刚开始都是主节点状态，但由于没有负责槽，所以不能接受任何读写操作，对于新节点的后续操作有两个选择：  
+ 为它迁移槽和数据实现扩容  
+ 作为其他主节点的从节点负责故障转移  
正式环境中建议使用redis-trib add-node命令加入新节点，该命令会在添加新节点之前检查状态，如果新加入的节点已经加入其他集群或者包含数据，则放弃加入集群  
加入新节点之前一定要确保新的节点没有数据并且没有加入任何集群，否则会造成数据丢失和错乱  
#### 迁移槽和数据  
##### 槽迁移计划  
槽是redis集群管理数据的基本单位，首先需要为新节点指定槽的迁移计划，确保原有节点的哪些槽迁移到新节点。迁移计划需要保证每个节点负责相似数量的槽，从而确保各节点的数据均匀  
##### 迁移数据  
数据迁移过程是逐个槽进行的  
流程说明：  
1. 对目标节点发送cluster setslots {solt} importing {sourceNodeId}命令，让目标节点主备导入槽的数据  
2. 对源节点发送cluster setslots {slot} migrating {targetNodeId}命令，让源节点主备迁出槽的数据  
3. 源节点循环执行cluster getKeysinslot {solt} {count}命令，获取count个属于槽{slot}的键  
4. 在源节点上执行migrate {targetIp} {targetPort} "" 0 {timeout} keys {keys...}命令，把获取的键通过流水线(pipeline)机制批量迁移到目标节点，批量迁移版本的migrate在redis3.0.6以上版本提供。对于大量key的场景，批量迁移键将极大降低节点之际的网络IO  
5. 重复步骤3、4，直到槽下所有的键值数据迁移到目标节点  
6. 向集群内所有主节点发送cluster setslots {solt} node {targetNodeId}命令，通知槽分配给目标节点。为了保证槽节点映射变更及时传播，需要遍历发送给所有主节点更新被迁移的槽指向向节点  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/14/1602669084579-1602669084629.png)  
redis-trib提供了槽重分片的功能，命令如下：  
`redis-trib.rb reshard host:port --from <arg> --solts <arg> --yes --t <arg> --pipeline <arg>`  
参数说明  
+ host:port:必传参数，集群内任意节点地址，用来获取整个集群信息  
+ --from 指定源节点的id，如果有多个源节点，使用逗号分隔，如果是all源节点变为集群内所有主节点，在迁移过程中提示用户输入  
+ --to 需要迁移的目标节点的id，目标节点只能填写一个，在迁移过程中提升用户输入  
+ --slots：需要迁移槽的总数量，在迁移过程中提示用户输入  
+ --yes 当打印reshard执行计划时，是否需要用户输入yes确认后再执行reshard  
+ --timeout：控制每次migrate操作的超时时间，默认60000毫秒  
+ pipeline：控制每次批量迁移键的数量，默认10  
#### 收缩集群  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/15/1602760435387-1602760435431.png)  
流程说明  
1. 首先需要确定下线节点是否有负责的槽，如果是，需要把槽迁移到其他节点，保证节点下线后整个集群槽节点映射的完整性  
2. 当下线节点不再负责槽或者本身是从节点时，就可以通知集群内其他节点忘记下线节点，当所有的节点忘记该节点后可以正常关闭  
##### 忘记节点  
redis提供了`cluster forget {downNodeId}`命令实现该功能  
当节点接收到cluster forget {downNodeId}命令后，会把nodeId指定的节点加入到禁用列表中，在禁用列表内的节点不再发送Gossip消息。禁用列表有效期时60秒，超过60秒节点会再次参与消息交换。也就是说，当第一次发送forget命令后，有60秒的时间让集群内的所有节点忘记下线节点  
一般不直接使用cluster forget 命令下线节点，需要更大量节点命令交互，建议使用redis-trib.rb del-node {host:port} {downNodeId} 命令  
### 请求路由  
redis集群对通信协议做了比较大的修改，为了追求性能的最大化，并没有采用代理的方式而是采用客户端直连节点的方式  
#### 请求重定向  
在集群模式下，redis接收任何键相关的命令时首先计算键对应的槽，再根据槽找出对应的节点，如果节点时自身，则处理键命令；否则回复MOVED重定向错误，通知客户端请求正确的节点，这个过程称为MOVED重定向  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/15/1602763437154-1602763437156.png)  
在redis-cli命令时，可以加入-c参数支持自动重定向，简化手动发起重定向操作  
这个过程是在redis-cli内部维护，实质上是client端接收到MOVED信息之后再次发起请求，并不在redis节点中完成请求转发  
键命令执行步骤主要分两步：计算槽、查找槽所对应的节点  
##### 计算槽  
redis首先需要计算槽所对应的键，根据键的有效部分使用CRC16函数计算出散列值，再取对16383的余数，使每个键都可以映射到0~16383槽范围内  
如果键内容包括{和}大括号字符，则计算槽的有效部分是括号内的内容；否则采用键的全部内容计算槽，其中键内使用大括号包含的内容又叫做hash_tag，它提供不同的键可以具备相同slot的功能  
例如在集群模式下使用mget等命令优化批量调用时，键列表必须具有相同的slot，否则会报错，这时可以利用hash_tag让不用的键具有相同的slot达到优化的目的  
但是对于hash_tag的使用也要注意，使数据的分布尽量均匀，避免对热数据使用hash_tag，导致请求分布不均匀  
##### 槽节点查找  
redis计算得到键对应的槽后，需要查找槽所对应的节点  
#### smart客户端  
##### smart客户端原理  
大多数开发语言的redis客户端都采用smart客户端，这是为了避免client客户端的MOVED重定向，smart客户端通过在内部维护slot->node的映射关系，本地就可以实现键到节点的查找，从而保证IO效率的最大化，而MOVED重定向负责协助smart客户端更新slot->node映射  
#### ASK重定向  
##### 客户端ASK重定向流程  
redis集群支持在线迁移槽(slot)和数据来完成水平伸缩，当slot对应的数据从源节点到目标节点的迁移过程中，客户端需要做到智能识别，保证键命令可以正常执行  
当出现上述情况时，客户端键命令执行流程将发生变化  
1. 客户端根据本地slots缓存发送命令到源节点，如果存在键对象则直接执行并返回结果给客户端  
2. 如果键对象不存在，则可能存在于目标节点，这是源节点会回复ASK重定向异常，格式：(error) ASK {slot} {targetIP}:{targetPort}  
3. 客户端从ASK重定向异常中提取目标节点信息，发送asking命令到目标节点打开客户端连接标识，再执行键命令。如果存在则执行，不存在则返回不存在信息  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/15/1602766378789-1602766378797.png)  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/15/1602766412077-1602766412078.png)  
+ ASK和MOVED虽然都是客户端的重定向控制，但是有着本质区别。ASK重定向说明集群正在进行slot数据迁移，客户端无法知道什么时候迁移完成，因此只能临时性的重定向，客户端不会更新slots缓存；MOVED重定向说明键对应的槽已经明确指定到新的节点，因此需要更新slots缓存  
## 故障转移  
### 故障发现  
故障发现也是通过消息传播机制实现的，主要环节包括主观下线(pfail)和客观下线(fail)  
+ 主观下线：指某个节点认为另一个节点不可用，即下线状态，这个状态并不是最终的故障判定，只能代表一个节点的意见，可能存在误判  
+ 客观下线：指标记一个节点真正的下线，集群内多个节点都认为该节点不可用，从而达成共识。如果是持有槽的主节点故障，需要为该节点进行故障转移  
##### 主观下线  
集群内每个节点都会再定期向其他节点发送ping消息，接收节点回复pong消息作为响应。如果在cluster-node-timeout时间内通信一直失败，则发送节点会认为接收节点存在故障，把接收节点标记为主观下线(pfail)  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/16/1602835080862-1602835080863.png)  
流程说明：  
1. 节点a发送ping消息给节点b，如果通信正常将接收到pong消息，节点a更新最近一次与节点b的通信时间  
2. 如果节点a与节点b通信出现问题则断开连接，下次会进行重连。如果一直通信失败，则节点a记录的与节点b最后通信时间将无法更新  
3. 节点a内的定时任务检测到与节点b最后通信时间超过cluster-node-timeout时，更新本地对节点b的状态为主观下线(pfail)  
简单来说，主观下线就是在cluster-node-timeout时间内某节点无法与另一个节点完成通信时，被该节点标记为主管下线(pfail)  
##### 客观下线  
当某个节点判断另一个节点主观下线后，相应的节点状态会跟随消息在集群内部传播。ping/pong消息的消息体会携带集群1/10的其他节点状态数据，当接受节点发现消息体中含有主观下线的节点状态时，会在本地找到故障节点ClusterNode结构，保存到下线报告链表中  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/16/1602836096540-1602836096542.png)  
流程说明：  
1. 当消息体内含有其他节点的pfail状态会判断发送节点的状态，如果发送节点是主节点则对报告的pfail状态处理，从节点则忽略  
2. 找到pfail对应的节点结构，更新ClusterNode内部下线报告链表  
3. 根据更新后的下线报告链表尝试进行客观下线  
每个下线报告都存在有效期，每次尝试触发客观下线时，都会检测下线报告是否过期，对于过期的下线报告将被删除。如果在cluster-node-time x 2的时间内该下线报告没有得到更新则过期并删除  
如果cluster-node-time x 2时间内无法收集到一半以上槽节点的下线报告，那么之前的下线报告也将会过期，也就是说主观下线上报速度追不上下线报告过期的速度，那么故障节点将永远无法被标记为客观下线从而导致故障转移失败。因此不建议将cluster-node-timeout设置的过小  
集群中的节点每次接收到其他节点的pfail状态，都会尝试触发客观下线  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/16/1602837734922-1602837734924.png)  
1. 首先统计有效的下线报告数量，如果小于集群内持有槽的主节点总数的一半则退出  
2. 当下线报告大于槽节点数量一半时，标记对应故障节点为客观下线状态  
3. 向集群广播一条fail消息，通知所有的节点将故障节点标记为客观下线，fail消息的消息体只包含故障节点的ID  
需要理解的是，尽管存在广播fail消息机制，但是集群所有节点直到故障节点进入客观下线状态是不确定的  
### 故障恢复  
故障节点变为客观下线后，如果下线节点是持有槽的主节点则需要在它的从节点中选出一个替换它，从而保证集群的高可用。下线主节点的所有从节点承担故障恢复的任务，当从节点通过内部定时任务发现自身复制的主节点进入客观下线时，将会触发故障恢复流程  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/16/1602838324110-1602838324111.png)  
##### 资格检查  
每个从节点都要检查最后与主节点断线时间，判断是否有资格替换故障的主节点。如果从节点与主节点断线时间超过cluster-node-time x cluster-slave-validity-factor，则当前从节点不具备故障转移资格。参数cluster-slave-validity-factor用于从节点的有效因子，默认为10  
##### 准备选举时间  
当从节点符合故障转移资格后，更新后触发故障选举的时间，只有到达该时间后才能执行后续的流程  
这里之所以采用延迟触发机制，主要时通过对多个从节点使用不同的延迟选举时间来支持优先级问题。复制偏移量越大说明从节点延迟越低，那么它应该具有更高的优先级来替换故障的主节点  
##### 发起选举  
当从节点定时任务检测到达故障选举时间(failover_auth_time)到达后，发起选举流程：  
1. 更新纪元配置  
配置纪元是一个只增不减的整数，每个主节点自身维护一个配置纪元(cluserNode.configEpoch)标示当前主节点的版本，所有主节点的配置纪元都不想等，从节点会复制主节点的配置纪元。整个集群又维护一个全局的配置纪元(clusterState.currentEpoch)，用于记录集群内所有主节点配置纪元的最大版本  
配置纪元的主要作用  
+ 标示集群内每个主节点的不同版本和当前集群最大版本  
+ 每次集群发生重要事件时，这里的重要时间指出现新的主节点(新加入的或者由从节点转换而来),从节点竞争选举，都会递增集群全局的配置纪元并赋值给相关主节点，用于记录这一关键时间  
+ 主节点具有更大的配置纪元代表了更新的集群状态，因此当节点间进行ping/pong消息交换时，以配置纪元更大的一方为准，防止过时的消息状态污染集群  
##### 选举投票  
只有持有槽的主节点才会处理故障选举消息(FAILOVER_AUTH_REQUEST),当接到第一个请求投票的从节点消息时回复FAILOVER_AUTH_ACK消息作为投票，之后相同配置纪元内其他从节点的选举消息将忽略  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/16/1602840630646-1602840630648.png)  
如果在开始投票之后的cluster-node-timeout x 2时间内从节点没有获取足够数量的投票，则本次选举作废。从节点对配置纪元自增并发起下一次投票，直到选举成功  
##### 替换主节点  
当从节点收集到足够的选票之后，触发替换主节点操作  
1. 当前从节点取消复制变为主节点  
2. 执行cluster DelSlot操作撤销故障主节点负责的槽，并执行cluser addSlot把这些槽委派给自己  
3. 向集群广播自己的pong消息，通知集群内所有节点当前从节点变为主节点并接管了故障主节点的槽消息  
### 故障转移时间  
故障转移时间  
1. 主观下线(pfail)识别时间=cluster-node-timeout  
2. 主观下线状态消息转播时间<=cluster-node-timeout/2  
3. 从节点转移时间<=1000毫秒  
因此故障转移时间跟cluster-node-timeout参数息息相关，默认15秒  
## 集群运维  
#### 集群完整性  
默认情况下当集群16384个槽任何一个没有指派到节点时整个集群不可用。执行任何键命令返回(error) CLUSTERDOWN Hash slot not serverd错误  
但是当持有槽的主节点下线时，从故障发现到自动完成转移期间整个集群是不可用状态，对于大多数业务无法容忍这种情况，因此建议将参数cluster-reqire-full-coverage配置为no，当主节点故障转移时只影响它负责槽的相关命令执行，不会影响其他主节点的可用性  
#### 带宽消耗  
节点间消息通信对带宽的消耗体现在以下几个方面  
+ 消息发送频率：跟cluster-node-timeout密切相关  
+ 消息数据量：每个消息主要的数据占用包含：slots槽数组(2KB)和整个集群1/10的状态数据(10个节点状态数据约1KB)  
+ 节点部署的机器规模：机器带宽的上线是固定的，因此相同规模的集群分布的机器越多每台机器划分的节点越均匀，则集群内整体的可用带宽越高  
#### Pub/Sub广播问题  
在集群模式下内部实现对所有的publish命令都会想所有的节点进行广播，造成每条publish数据都会在集群内所有节点传播因此，加重带宽负担  
![title](https://gitee.com/liujinxi931204/image/raw/master/gitnote/2020/10/19/1603097952069-1603097952071.png)  
针对集群模式下的publish广播问题，当频繁应用Pub/Sub功能时应该避免在大量节点的集群内使用，否则会严重消耗集群内网络带宽  
#### 集群倾斜  
集群倾斜指不同节点之间的数量和请求量出现明显差异，这种情况将加大负载均衡和开发运维难度  
##### 数据倾斜  
数据倾斜主要分为以下几种  
+ 节点和槽分配严重不均  
+ 不同槽对应键数量差异过大  
+ 集合对象包含大量元素  
+ 内存相关配置不一致  
1. 节点和槽分配严重不均  
针对每个节点分配的槽不均的情况，可以使用redis-trib.rb info {host:port}进行定位  
当节点对应的槽数量不均匀时，可以使用redis-trib.rb rebalance命令进行平衡  
2. 不同槽对应键数量差异过大  
正常情况下槽内键的数量会相对均匀，但当大量使用hash_tag时，会产生不同的键映射到同一个槽的情况。通过命令  
`cluster countkeysinslot {slot}`可以获取槽对应的键数量  
`cluster getkeysinslot {slot} {count}`循环迭代出槽下所有的键  
3. 集合包含大量元素  
对于大集合对象的识别可以使用redis-cli --bigkeys命令识别  
同时集群槽数据迁移是对键执行migrate操作完成，过大的键集容易导致migrate命令超时导致迁移失败  
4. 内存相关配置不一致。内存相关配置指hash-max-ziplist-value、set-max-inset-entries等压缩数据结构配置。当集群大量使用hash、set等数据结构时，如果内存压缩数据结构配置不一致，从而造成节点内存量倾斜  
##### 请求倾斜  
集群内特定节点请求量/流量过大导致节点之间负载不均，影响集群均衡和运维成本  
+ 合理涉及键，热点大集合对象做拆分或者使用hmegt代替hgetall避免整体读取  
+ 不用使用热键作为hash_tag，避免映射到同一槽  
+ 对于一致性要求不高的场景，客户端使用本地缓存减少热键调用  
#### 集群读写分离  
##### 只读连接  
集群模式下从节点不接受任何读写请求，发送过来的键命令会重定向到负责槽的主节点上(其中包括他的主节点)。当需要使用从节点分担主节点读压力时，可以使用readonly命令打开客户端连接只读状态。之前的复制配置salve-read-only在集群模式下无效。当开启只读状态时，从节点接收到读命令的处理流程变为：如果对应的槽属于自己正在复制的主节点则直接执行命令，否则返回重定向信息  
readonly命令是连接级别生效，因此每次新建连接都需要执行readonly开启只读状态，执行readwrite命令可以关闭只读连接  
##### 读写分离  
集群模式下的读写分离依然会遇到：复制延迟、读取过期数据、从节点故障等问题  
针对从节点故障问题，客户端需要维护可用节点列表，集群提供了`cluster slaves {nodeId}`命令，返回nodeId对应主节点下所有从节点信息  
集群模式下读写分离涉及对客户端修改如下：  
+ 维护每个主节点可用从节点列表  
+ 针对读命令维护请求路由  
+ 从节点新建连接开启readonly状态  
集群模式下读写分离成本比较高，可用直接扩展主节点数量提高集群性能，一般不建议集群模式下读写分离  

